"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π
"""

import os
import logging
import requests
from pathlib import Path
from typing import Optional
from urllib.parse import urlparse
from tqdm import tqdm
from huggingface_hub import hf_hub_download, snapshot_download

logger = logging.getLogger(__name__)


def download_file_with_progress(url: str, local_path: str) -> bool:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

    Args:
        url: URL —Ñ–∞–π–ª–∞
        local_path: –õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è

    Returns:
        True –µ—Å–ª–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ
    """
    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()

        total_size = int(response.headers.get("content-length", 0))

        os.makedirs(os.path.dirname(local_path), exist_ok=True)

        with open(local_path, "wb") as file, tqdm(
            desc=f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ {os.path.basename(local_path)}",
            total=total_size,
            unit="B",
            unit_scale=True,
            unit_divisor=1024,
        ) as progress_bar:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    file.write(chunk)
                    progress_bar.update(len(chunk))

        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞: {local_path}")
        return True

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {url}: {e}")
        if os.path.exists(local_path):
            os.remove(local_path)
        return False


def download_llm_model_from_hf(
    model_repo: str, filename: str, local_dir: str, cache_dir: Optional[str] = None
) -> Optional[str]:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç LLM –º–æ–¥–µ–ª—å —Å Hugging Face Hub

    Args:
        model_repo: –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "microsoft/DialoGPT-medium")
        filename: –ò–º—è —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "model.gguf")
        local_dir: –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        cache_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫—ç—à–∞ HF

    Returns:
        –ü—É—Ç—å –∫ —Å–∫–∞—á–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏–ª–∏ None
    """
    try:
        logger.info(f"üîÑ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ LLM –º–æ–¥–µ–ª–∏ {model_repo}/{filename}...")

        # –ü–æ–ø—ã—Ç–∫–∞ —Å–∫–∞—á–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ñ–∞–π–ª
        model_path = hf_hub_download(
            repo_id=model_repo,
            filename=filename,
            local_dir=local_dir,
            cache_dir=cache_dir,
        )
        logger.info(f"‚úÖ LLM –º–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞: {model_path}")
        return model_path

    except Exception as e:
        logger.warning(
            f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å —Ç–æ—á–Ω—ã–π —Ñ–∞–π–ª '{filename}' –∏–∑ {model_repo}: {e}. –ü—Ä–æ–±—É–µ–º snapshot_download –∏ –∞–≤—Ç–æ–ø–æ–¥–±–æ—Ä .gguf"
        )

        try:
            # –°–∫–∞—á–∏–≤–∞–µ–º –≤–µ—Å—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π (—Ç–æ–ª—å–∫–æ .gguf) –∏ –≤—ã–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–π –∫–≤–∞–Ω—Ç
            repo_dir = snapshot_download(
                repo_id=model_repo,
                cache_dir=cache_dir,
                allow_patterns=["*.gguf", "**/*.gguf"],
                local_files_only=False,
            )

            # –ò—â–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–π GGUF (–ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –ø–æ –ø–æ–¥—Å—Ç—Ä–æ–∫–µ –∏–∑ filename)
            target_hint = None
            name_lower = filename.lower()
            if "q6_k" in name_lower:
                target_hint = "q6_k"
            elif "q5" in name_lower:
                target_hint = "q5"
            elif "q4" in name_lower:
                target_hint = "q4"

            candidates: list[Path] = []
            for p in Path(repo_dir).rglob("*.gguf"):
                candidates.append(p)

            if not candidates:
                logger.error("‚ùå –í snapshot –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ .gguf —Ñ–∞–π–ª–æ–≤")
                return None

            def score(p: Path) -> tuple[int, int]:
                s = p.name.lower()
                # –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –∫–≤–∞–Ω—Ç–æ–≤–∫–∏, –∑–∞—Ç–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
                match = 1 if (target_hint and target_hint in s) else 0
                return (match, p.stat().st_size)

            best = sorted(candidates, key=score, reverse=True)[0]

            # –ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å/—Å—Å—ã–ª–∞—Ç—å—Å—è –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥
            os.makedirs(local_dir, exist_ok=True)
            dest = Path(local_dir) / best.name
            if str(best.resolve()) != str(dest.resolve()):
                try:
                    import shutil

                    shutil.copy2(best, dest)
                except Exception:
                    # –µ—Å–ª–∏ –∫–æ–ø–∏—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞ (—Å–∫–≤–æ–∑–Ω–æ–π volume), –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø—É—Ç—å
                    dest = best

            logger.info(f"‚úÖ –í—ã–±—Ä–∞–Ω GGUF —Ñ–∞–π–ª: {dest}")
            return str(dest)

        except Exception as e2:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ snapshot_download –∏–ª–∏ –ø–æ–¥–±–æ—Ä–∞ .gguf: {e2}")
            return None


def download_embedding_model(model_name: str, cache_dir: Optional[str] = None) -> bool:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç embedding –º–æ–¥–µ–ª—å —Å Hugging Face

    Args:
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        cache_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫—ç—à–∞

    Returns:
        True –µ—Å–ª–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ
    """
    try:
        logger.info(f"üîÑ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ embedding –º–æ–¥–µ–ª–∏ {model_name}...")

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –∫—ç—à–∞
        if cache_dir:
            os.environ["TRANSFORMERS_CACHE"] = cache_dir
            os.environ["HF_HOME"] = cache_dir

        # –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –ø–æ–ª–Ω–æ—Å—Ç—å—é
        snapshot_download(
            repo_id=model_name, cache_dir=cache_dir, local_files_only=False
        )

        logger.info(f"‚úÖ Embedding –º–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞: {model_name}")
        return True

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è embedding –º–æ–¥–µ–ª–∏: {e}")
        return False


# –ü—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
RECOMMENDED_MODELS = {
    "llm": {
        "gpt-oss-20b": {
            "repo": "unsloth/gpt-oss-20b-GGUF",
            "filename": "gpt-oss-20b-Q6_K.gguf",
            "description": "OpenAI gpt-oss-20b (GGUF, Q6_K) –æ—Ç Unsloth",
        },
        "vikhr-7b-instruct": {
            "repo": "oblivious/Vikhr-7B-instruct-GGUF",
            "filename": "Vikhr-7B-instruct-Q4_K_M.gguf",
            "description": "Vikhr 7B - —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –æ—Ç Vikhrmodels",
        },
        "qwen2.5-7b-instruct": {
            "repo": "Qwen/Qwen2.5-7B-Instruct-GGUF",
            "filename": "qwen2.5-7b-instruct-q4_k_m.gguf",
            "description": "Qwen2.5 7B - –æ—Ç–ª–∏—á–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞",
        },
        "saiga-mistral-7b": {
            "repo": "IlyaGusev/saiga_mistral_7b_gguf",
            "filename": "model-q4_K.gguf",
            "description": "Saiga Mistral 7B - —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ",
        },
        "openchat-3.6-8b": {
            "repo": "openchat/openchat-3.6-8b-20240522-GGUF",
            "filename": "openchat-3.6-8b-20240522-q4_k_m.gguf",
            "description": "OpenChat 3.6 8B - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å",
        },
    },
    "embedding": {
        "multilingual-e5-large": {
            "name": "intfloat/multilingual-e5-large",
            "description": "–õ—É—á—à–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è embedding –º–æ–¥–µ–ª—å",
        },
        "multilingual-mpnet": {
            "name": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
            "description": "–ë—ã—Å—Ç—Ä–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å",
        },
        "bge-m3": {
            "name": "BAAI/bge-m3",
            "description": "BGE M3 - –æ—Ç–ª–∏—á–Ω–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å",
        },
    },
}


def auto_download_models(
    llm_model_key: str = "gpt-oss-20b",
    embedding_model_key: str = "multilingual-e5-large",
    models_dir: str = "/models",
    cache_dir: Optional[str] = None,
) -> tuple[Optional[str], bool]:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∫–∞—á–∏–≤–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏

    Returns:
        (–ø—É—Ç—å_–∫_llm_–º–æ–¥–µ–ª–∏, —É—Å–ø–µ—Ö_embedding_–º–æ–¥–µ–ª–∏)
    """
    llm_path = None
    embedding_success = False

    # –°–∫–∞—á–∏–≤–∞–µ–º LLM –º–æ–¥–µ–ª—å
    if llm_model_key in RECOMMENDED_MODELS["llm"]:
        llm_config = RECOMMENDED_MODELS["llm"][llm_model_key]
        logger.info(f"üì• {llm_config['description']}")

        llm_path = download_llm_model_from_hf(
            model_repo=llm_config["repo"],
            filename=llm_config["filename"],
            local_dir=models_dir,
            cache_dir=cache_dir,
        )

    # –°–∫–∞—á–∏–≤–∞–µ–º embedding –º–æ–¥–µ–ª—å
    if embedding_model_key in RECOMMENDED_MODELS["embedding"]:
        embedding_config = RECOMMENDED_MODELS["embedding"][embedding_model_key]
        logger.info(f"üì• {embedding_config['description']}")

        embedding_success = download_embedding_model(
            model_name=embedding_config["name"], cache_dir=cache_dir
        )

    return llm_path, embedding_success
