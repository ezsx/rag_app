"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π
"""

import os
import logging
import requests
from pathlib import Path
from typing import Optional
from urllib.parse import urlparse
from tqdm import tqdm
from huggingface_hub import hf_hub_download, snapshot_download

logger = logging.getLogger(__name__)


def download_file_with_progress(url: str, local_path: str) -> bool:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

    Args:
        url: URL —Ñ–∞–π–ª–∞
        local_path: –õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è

    Returns:
        True –µ—Å–ª–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ
    """
    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()

        total_size = int(response.headers.get("content-length", 0))

        os.makedirs(os.path.dirname(local_path), exist_ok=True)

        with open(local_path, "wb") as file, tqdm(
            desc=f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ {os.path.basename(local_path)}",
            total=total_size,
            unit="B",
            unit_scale=True,
            unit_divisor=1024,
        ) as progress_bar:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    file.write(chunk)
                    progress_bar.update(len(chunk))

        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞: {local_path}")
        return True

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {url}: {e}")
        if os.path.exists(local_path):
            os.remove(local_path)
        return False


def download_llm_model_from_hf(
    model_repo: str, filename: str, local_dir: str, cache_dir: Optional[str] = None
) -> Optional[str]:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç LLM –º–æ–¥–µ–ª—å —Å Hugging Face Hub

    Args:
        model_repo: –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "microsoft/DialoGPT-medium")
        filename: –ò–º—è —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "model.gguf")
        local_dir: –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        cache_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫—ç—à–∞ HF

    Returns:
        –ü—É—Ç—å –∫ —Å–∫–∞—á–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏–ª–∏ None
    """
    try:
        logger.info(f"üîÑ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ LLM –º–æ–¥–µ–ª–∏ {model_repo}/{filename}...")

        # –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ HuggingFace Hub
        model_path = hf_hub_download(
            repo_id=model_repo,
            filename=filename,
            local_dir=local_dir,
            cache_dir=cache_dir,
        )

        logger.info(f"‚úÖ LLM –º–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞: {model_path}")
        return model_path

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è LLM –º–æ–¥–µ–ª–∏: {e}")
        return None


def download_embedding_model(model_name: str, cache_dir: Optional[str] = None) -> bool:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç embedding –º–æ–¥–µ–ª—å —Å Hugging Face

    Args:
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        cache_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫—ç—à–∞

    Returns:
        True –µ—Å–ª–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ
    """
    try:
        logger.info(f"üîÑ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ embedding –º–æ–¥–µ–ª–∏ {model_name}...")

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –∫—ç—à–∞
        if cache_dir:
            os.environ["TRANSFORMERS_CACHE"] = cache_dir
            os.environ["HF_HOME"] = cache_dir

        # –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –ø–æ–ª–Ω–æ—Å—Ç—å—é
        snapshot_download(
            repo_id=model_name, cache_dir=cache_dir, local_files_only=False
        )

        logger.info(f"‚úÖ Embedding –º–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞: {model_name}")
        return True

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è embedding –º–æ–¥–µ–ª–∏: {e}")
        return False


# –ü—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
RECOMMENDED_MODELS = {
    "llm": {
        "vikhr-7b-instruct": {
            "repo": "oblivious/Vikhr-7B-instruct-GGUF",
            "filename": "Vikhr-7B-instruct-Q4_K_M.gguf",
            "description": "Vikhr 7B - —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –æ—Ç Vikhrmodels",
        },
        "qwen2.5-7b-instruct": {
            "repo": "Qwen/Qwen2.5-7B-Instruct-GGUF",
            "filename": "qwen2.5-7b-instruct-q4_k_m.gguf",
            "description": "Qwen2.5 7B - –æ—Ç–ª–∏—á–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞",
        },
        "saiga-mistral-7b": {
            "repo": "IlyaGusev/saiga_mistral_7b_gguf",
            "filename": "model-q4_K.gguf",
            "description": "Saiga Mistral 7B - —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ",
        },
        "openchat-3.6-8b": {
            "repo": "openchat/openchat-3.6-8b-20240522-GGUF",
            "filename": "openchat-3.6-8b-20240522-q4_k_m.gguf",
            "description": "OpenChat 3.6 8B - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å",
        },
    },
    "embedding": {
        "multilingual-e5-large": {
            "name": "intfloat/multilingual-e5-large",
            "description": "–õ—É—á—à–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è embedding –º–æ–¥–µ–ª—å",
        },
        "multilingual-mpnet": {
            "name": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
            "description": "–ë—ã—Å—Ç—Ä–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å",
        },
        "bge-m3": {
            "name": "BAAI/bge-m3",
            "description": "BGE M3 - –æ—Ç–ª–∏—á–Ω–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å",
        },
    },
}


def auto_download_models(
    llm_model_key: str = "vikhr-7b-instruct",
    embedding_model_key: str = "multilingual-e5-large",
    models_dir: str = "/models",
    cache_dir: Optional[str] = None,
) -> tuple[Optional[str], bool]:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∫–∞—á–∏–≤–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏

    Returns:
        (–ø—É—Ç—å_–∫_llm_–º–æ–¥–µ–ª–∏, —É—Å–ø–µ—Ö_embedding_–º–æ–¥–µ–ª–∏)
    """
    llm_path = None
    embedding_success = False

    # –°–∫–∞—á–∏–≤–∞–µ–º LLM –º–æ–¥–µ–ª—å
    if llm_model_key in RECOMMENDED_MODELS["llm"]:
        llm_config = RECOMMENDED_MODELS["llm"][llm_model_key]
        logger.info(f"üì• {llm_config['description']}")

        llm_path = download_llm_model_from_hf(
            model_repo=llm_config["repo"],
            filename=llm_config["filename"],
            local_dir=models_dir,
            cache_dir=cache_dir,
        )

    # –°–∫–∞—á–∏–≤–∞–µ–º embedding –º–æ–¥–µ–ª—å
    if embedding_model_key in RECOMMENDED_MODELS["embedding"]:
        embedding_config = RECOMMENDED_MODELS["embedding"][embedding_model_key]
        logger.info(f"üì• {embedding_config['description']}")

        embedding_success = download_embedding_model(
            model_name=embedding_config["name"], cache_dir=cache_dir
        )

    return llm_path, embedding_success
