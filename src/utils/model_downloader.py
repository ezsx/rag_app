"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π
"""

import os
import logging
import requests
from pathlib import Path
from typing import Optional, List
from urllib.parse import urlparse
from tqdm import tqdm
from huggingface_hub import hf_hub_download, snapshot_download, HfApi
from huggingface_hub.utils import logging as hf_logging

logger = logging.getLogger(__name__)


def download_file_with_progress(url: str, local_path: str) -> bool:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

    Args:
        url: URL —Ñ–∞–π–ª–∞
        local_path: –õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è

    Returns:
        True –µ—Å–ª–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ
    """
    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()

        total_size = int(response.headers.get("content-length", 0))

        os.makedirs(os.path.dirname(local_path), exist_ok=True)

        with open(local_path, "wb") as file, tqdm(
            desc=f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ {os.path.basename(local_path)}",
            total=total_size,
            unit="B",
            unit_scale=True,
            unit_divisor=1024,
        ) as progress_bar:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    file.write(chunk)
                    progress_bar.update(len(chunk))

        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞: {local_path}")
        return True

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {url}: {e}")
        if os.path.exists(local_path):
            os.remove(local_path)
        return False


# ---- LLM: —Ç–æ—á–µ—á–Ω–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ, –±–µ–∑ snapshot ----

_PREFERRED_ORDER = [
    "q8_0",  # —è–≤–Ω–æ —Ö–æ—Ç–∏–º 8-bit
    "q6_k",
    "q5_k_m",
    "q5_k_s",
    "q4_k_m",
    "q4_k_s",
    "q4_0",
]


def _list_repo_gguf(repo_id: str) -> List[str]:
    api = HfApi()
    files = [
        f.rfilename
        for f in api.list_files_info(repo_id, expand=True)
        if f.rfilename.lower().endswith(".gguf")
    ]
    # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ (–±–µ–∑ –ø—É—Ç–µ–π)
    return [Path(f).name for f in files]


def _pick_best_filename(available: List[str], hint_filename: str) -> Optional[str]:
    if not available:
        return None
    avail_lower = [s.lower() for s in available]
    hint_lower = hint_filename.lower()

    # 1) —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –∏–º–µ–Ω–∏
    for i, s in enumerate(avail_lower):
        if s == hint_lower:
            return available[i]

    # 2) –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ PREFERRED_ORDER —Å —É—á—ë—Ç–æ–º —Ö–∏–Ω—Ç–∞
    def rank(fname: str) -> tuple[int, int, int]:
        s = fname.lower()
        # —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –∫–ª—é—á–µ–≤—É—é –ø–æ–¥—Å–∫–∞–∑–∫—É (q8/q6/q5/q4) –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏–º–µ–Ω–∏
        hint_score = (
            1
            if any(k in hint_lower and k in s for k in ["q8_0", "q6_k", "q5", "q4"])
            else 0
        )
        # –∏–Ω–¥–µ–∫—Å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è (–Ω–∏–∂–µ ‚Äî –ª—É—á—à–µ)
        pref_idx = next((i for i, k in enumerate(_PREFERRED_ORDER) if k in s), 999)
        # –¥–µ—Ç–µ—Ä–º–∏–Ω–∏–∑–∞—Ü–∏—è
        return (hint_score, -int(1e6 - pref_idx), -len(s))

    best = sorted(available, key=rank, reverse=True)[0]
    return best


def download_llm_model_from_hf(
    model_repo: str, filename: str, local_dir: str, cache_dir: Optional[str] = None
) -> Optional[str]:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –æ–¥–∏–Ω GGUF –∏–∑ Hugging Face Hub –±–µ–∑ snapshot.
    –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ—Ç—Å—è —Ç–æ—á–Ω–æ–µ –∏–º—è, –∑–∞—Ç–µ–º ‚Äî –ø–æ–¥–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç.
    """
    # —Ö–æ—Ç–∏–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –ª–æ–≥ –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å –æ—Ç HF
    hf_logging.set_verbosity_info()
    os.environ.setdefault("HF_HUB_DISABLE_PROGRESS_BARS", "0")
    os.environ.setdefault("HF_HUB_ENABLE_HF_TRANSFER", "1")

    os.makedirs(local_dir, exist_ok=True)

    def _download(fname: str) -> Optional[str]:
        logger.info(f"üîÑ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ LLM –º–æ–¥–µ–ª–∏ {model_repo}/{fname}...")
        return hf_hub_download(
            repo_id=model_repo,
            filename=fname,
            local_dir=local_dir,
            cache_dir=cache_dir,
            resume_download=True,
        )

    # 1) –ø—Ä–æ–±—É–µ–º —Ç–æ—á–Ω–æ–µ –∏–º—è
    try:
        path = _download(filename)
        logger.info(f"‚úÖ LLM –º–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞: {path}")
        return path
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å '{filename}' –∏–∑ {model_repo}: {e}")

    # 2) –±–µ–∑ snapshot ‚Äî —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏ –ø–æ–¥–±–æ—Ä
    try:
        files = _list_repo_gguf(model_repo)
        if not files:
            logger.error("‚ùå –í —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –Ω–µ—Ç *.gguf")
            return None

        picked = _pick_best_filename(files, filename)
        if not picked:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–æ–±—Ä–∞—Ç—å GGUF")
            return None

        logger.info(f"‚û°Ô∏è –í—ã–±—Ä–∞–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã–π GGUF: {picked}")
        path = _download(picked)
        logger.info(f"‚úÖ LLM –º–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞: {path}")
        return path

    except Exception as e2:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–±–æ—Ä–∞/—Å–∫–∞—á–∏–≤–∞–Ω–∏—è GGUF: {e2}")
        return None


# ---- Embedding / Reranker (–æ—Å—Ç–∞–≤–ª—è–µ–º snapshot, —ç—Ç–æ –Ω–æ—Ä–º) ----


def download_embedding_model(model_name: str, cache_dir: Optional[str] = None) -> bool:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç embedding –º–æ–¥–µ–ª—å —Å Hugging Face

    Args:
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        cache_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫—ç—à–∞

    Returns:
        True –µ—Å–ª–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ
    """
    try:
        logger.info(f"üîÑ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ embedding –º–æ–¥–µ–ª–∏ {model_name}...")
        if cache_dir:
            os.environ["TRANSFORMERS_CACHE"] = cache_dir
            os.environ["HF_HOME"] = cache_dir

        snapshot_download(
            repo_id=model_name, cache_dir=cache_dir, local_files_only=False
        )

        logger.info(f"‚úÖ Embedding –º–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞: {model_name}")
        return True

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è embedding –º–æ–¥–µ–ª–∏: {e}")
        return False


# –ü—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
RECOMMENDED_MODELS = {
    "llm": {
        "gpt-oss-20b": {
            "repo": "unsloth/gpt-oss-20b-GGUF",
            "filename": "gpt-oss-20b-Q6_K.gguf",
            "description": "OpenAI gpt-oss-20b (GGUF, Q6_K) –æ—Ç Unsloth",
        },
        "vikhr-7b-instruct": {
            "repo": "oblivious/Vikhr-7B-instruct-GGUF",
            "filename": "Vikhr-7B-instruct-Q4_K_M.gguf",
            "description": "Vikhr 7B - —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –æ—Ç Vikhrmodels",
        },
        # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π bartowski –∏ —Ñ–∏–∫—Å–∏—Ä—É–µ–º Q8_0
        "qwen2.5-7b-instruct": {
            "repo": "bartowski/Qwen2.5-7B-Instruct-GGUF",
            "filename": "Qwen2.5-7B-Instruct-Q8_0.gguf",
            "description": "Qwen2.5 7B Instruct (GGUF, Q8_0) ‚Äî –±—ã—Å—Ç—Ä–æ –∑–∞–º–µ–Ω–∏—Ç—å –±–µ–∑ snapshot",
        },
        "qwen2.5-3b-instruct": {
            "repo": "Qwen/Qwen2.5-3B-Instruct-GGUF",
            "filename": "qwen2.5-3b-instruct-q4_k_m.gguf",
            "description": "Qwen2.5 3B Instruct (GGUF, Q4_K_M)",
        },
        "saiga-mistral-7b": {
            "repo": "IlyaGusev/saiga_mistral_7b_gguf",
            "filename": "model-q4_K.gguf",
            "description": "Saiga Mistral 7B - —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ",
        },
        "openchat-3.6-8b": {
            "repo": "openchat/openchat-3.6-8b-20240522-GGUF",
            "filename": "openchat-3.6-8b-20240522-q4_k_m.gguf",
            "description": "OpenChat 3.6 8B - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å",
        },
    },
    "embedding": {
        "multilingual-e5-large": {
            "name": "intfloat/multilingual-e5-large",
            "description": "–õ—É—á—à–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è embedding –º–æ–¥–µ–ª—å",
        },
        "multilingual-mpnet": {
            "name": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
            "description": "–ë—ã—Å—Ç—Ä–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å",
        },
        "bge-m3": {
            "name": "BAAI/bge-m3",
            "description": "BGE M3 - –æ—Ç–ª–∏—á–Ω–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å",
        },
    },
    "reranker": {
        # –ö–ª—é—á–∏ –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏; –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π repo id
        "bge-reranker-v2-m3": {
            "name": "BAAI/bge-reranker-v2-m3",
            "description": "BAAI bge-reranker-v2-m3 (CrossEncoder, CPU)",
        }
    },
}


def auto_download_models(
    llm_model_key: str = "gpt-oss-20b",
    embedding_model_key: str = "multilingual-e5-large",
    models_dir: str = "/models",
    cache_dir: Optional[str] = None,
) -> tuple[Optional[str], bool]:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∫–∞—á–∏–≤–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏

    Returns:
        (–ø—É—Ç—å_–∫_llm_–º–æ–¥–µ–ª–∏, —É—Å–ø–µ—Ö_embedding_–º–æ–¥–µ–ª–∏)
    """
    llm_path = None
    embedding_success = False

    # –°–∫–∞—á–∏–≤–∞–µ–º LLM –º–æ–¥–µ–ª—å
    if llm_model_key in RECOMMENDED_MODELS["llm"]:
        llm_config = RECOMMENDED_MODELS["llm"][llm_model_key]
        logger.info(f"üì• {llm_config['description']}")
        llm_path = download_llm_model_from_hf(
            model_repo=llm_config["repo"],
            filename=llm_config["filename"],
            local_dir=models_dir,
            cache_dir=cache_dir,
        )

    # –°–∫–∞—á–∏–≤–∞–µ–º embedding –º–æ–¥–µ–ª—å
    if embedding_model_key in RECOMMENDED_MODELS["embedding"]:
        embedding_config = RECOMMENDED_MODELS["embedding"][embedding_model_key]
        logger.info(f"üì• {embedding_config['description']}")
        embedding_success = download_embedding_model(
            model_name=embedding_config["name"], cache_dir=cache_dir
        )

    return llm_path, embedding_success


def download_reranker_model(model_name: str, cache_dir: Optional[str] = None) -> bool:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç CrossEncoder —Ä–µ—Ä–µ–π–∫–µ—Ä –∏–∑ Hugging Face, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ.

    Args:
        model_name: repo id, –Ω–∞–ø—Ä–∏–º–µ—Ä "BAAI/bge-reranker-v2-m3"
        cache_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫—ç—à–∞ HF

    Returns:
        True –µ—Å–ª–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ
    """
    try:
        logger.info(f"üîÑ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ reranker –º–æ–¥–µ–ª–∏ {model_name}...")
        # –ö–ª–∞–¥–µ–º –≤ –∫—ç—à HF (TRANSFORMERS_CACHE/HF_HOME —á–∏—Ç–∞—é—Ç—Å—è –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞)
        snapshot_download(
            repo_id=model_name, cache_dir=cache_dir, local_files_only=False
        )
        logger.info(f"‚úÖ Reranker –º–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞: {model_name}")
        return True
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è reranker –º–æ–¥–µ–ª–∏: {e}")
        return False
