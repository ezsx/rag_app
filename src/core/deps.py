import os
import logging
from functools import lru_cache
from pathlib import Path
from typing import Optional
from fastapi import Depends, HTTPException
import chromadb
from llama_cpp import Llama

from adapters.chroma import Retriever
from services.qa_service import QAService
from services.query_planner_service import QueryPlannerService
from utils.model_downloader import auto_download_models, RECOMMENDED_MODELS
from core.settings import get_settings, Settings

logger = logging.getLogger(__name__)


@lru_cache
def get_chroma_client():
    """–°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç ChromaDB –∫–ª–∏–µ–Ω—Ç"""
    settings = get_settings()
    try:
        # –ü–æ–ø—Ä–æ–±—É–µ–º HTTP –∫–ª–∏–µ–Ω—Ç (–¥–ª—è Docker compose)
        client = chromadb.HttpClient(
            host=settings.chroma_host, port=settings.chroma_port
        )
        # –ü—Ä–æ–≤–µ—Ä–∏–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
        client.heartbeat()
        logger.info(
            f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ChromaDB HTTP: {settings.chroma_host}:{settings.chroma_port}"
        )
        return client
    except Exception as e:
        logger.warning(f"HTTP –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å ({e}), –ø—Ä–æ–±—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç")
        # Fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç
        return chromadb.PersistentClient(path=settings.chroma_path)


@lru_cache
def get_retriever() -> Retriever:
    settings = get_settings()
    client = get_chroma_client()
    """–°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç Retriever –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ ChromaDB"""
    embedding_model_key = settings.current_embedding_key
    collection_name = settings.current_collection

    # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if embedding_model_key in RECOMMENDED_MODELS["embedding"]:
        embedding_model = RECOMMENDED_MODELS["embedding"][embedding_model_key]["name"]
    else:
        # Fallback –Ω–∞ –ø—Ä—è–º–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        embedding_model = os.getenv("EMBEDDING_MODEL", "intfloat/multilingual-e5-large")

    logger.info(
        f"–ò—Å–ø–æ–ª—å–∑—É–µ–º embedding –º–æ–¥–µ–ª—å: {embedding_model} (–∫–æ–ª–ª–µ–∫—Ü–∏—è: {collection_name})"
    )

    # –ê–≤—Ç–æ—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ embedding –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
    auto_download_embedding = (
        os.getenv("AUTO_DOWNLOAD_EMBEDDING", "true").lower() == "true"
    )
    if auto_download_embedding:
        try:
            from utils.model_downloader import download_embedding_model

            download_embedding_model(embedding_model, settings.cache_dir)
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å embedding –º–æ–¥–µ–ª—å: {e}")

    return Retriever(client, collection_name, embedding_model)


@lru_cache
def get_llm():
    """–°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç LLM –º–æ–¥–µ–ª—å —Å –∞–≤—Ç–æ—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ–º"""
    settings = get_settings()
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
    llm_model_key = settings.current_llm_key
    models_dir = settings.models_dir
    cache_dir = settings.cache_dir
    auto_download = os.getenv("AUTO_DOWNLOAD_LLM", "true").lower() == "true"

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    n_gpu_layers = int(os.getenv("LLM_GPU_LAYERS", "-1"))
    n_ctx = int(os.getenv("LLM_CONTEXT_SIZE", "4096"))
    n_threads = int(os.getenv("LLM_THREADS", "8"))

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
    if llm_model_key in RECOMMENDED_MODELS["llm"]:
        model_config = RECOMMENDED_MODELS["llm"][llm_model_key]
        model_filename = model_config["filename"]
        model_path = os.path.join(models_dir, model_filename)
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –º–æ–¥–µ–ª—å: {model_config['description']}")
    else:
        # Fallback –Ω–∞ –ø—Ä—è–º–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ –ø—É—Ç–∏
        model_path = os.getenv("LLM_MODEL_PATH", f"{models_dir}/gpt-oss-20b-Q6_K.gguf")
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏: {model_path}")

    # –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ –ø—É—Ç–∏
    try:
        import llama_cpp as _ll

        logger.info(f"llama_cpp version: {getattr(_ll, '__version__', 'unknown')}")
    except Exception as _:
        logger.info("llama_cpp version: <unavailable>")
    logger.info(
        f"ENV: CUDA_VISIBLE_DEVICES={os.getenv('CUDA_VISIBLE_DEVICES')}, "
        f"LLM_GPU_LAYERS={os.getenv('LLM_GPU_LAYERS', str(n_gpu_layers))}, "
        f"LLM_CONTEXT_SIZE={os.getenv('LLM_CONTEXT_SIZE', str(n_ctx))}, "
        f"LLM_THREADS={os.getenv('LLM_THREADS', str(n_threads))}"
    )
    logger.info(f"–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏: {model_path}")
    if os.path.exists(model_path):
        try:
            sz = Path(model_path).stat().st_size
            logger.info(f"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏: {sz / (1024**3):.2f} GB")
        except Exception:
            pass

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    if not os.path.exists(model_path):
        if auto_download and llm_model_key in RECOMMENDED_MODELS["llm"]:
            logger.info(f"üîÑ –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∑–∞–ø—É—Å–∫–∞–µ–º –∞–≤—Ç–æ—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ...")

            # –ê–≤—Ç–æ—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
            downloaded_path, _ = auto_download_models(
                llm_model_key=llm_model_key,
                embedding_model_key="",  # –ù–µ —Å–∫–∞—á–∏–≤–∞–µ–º embedding –∑–¥–µ—Å—å
                models_dir=models_dir,
                cache_dir=cache_dir,
            )

            if downloaded_path and os.path.exists(downloaded_path):
                model_path = downloaded_path
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–∞: {model_path}")
            else:
                raise HTTPException(
                    status_code=503,
                    detail=f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å LLM –º–æ–¥–µ–ª—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É.",
                )
        else:
            raise FileNotFoundError(f"LLM model not found at {model_path}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    try:
        logger.info(f"üìö –ó–∞–≥—Ä—É–∂–∞–µ–º LLM –º–æ–¥–µ–ª—å: {os.path.basename(model_path)}")
        logger.info(
            f"   GPU —Å–ª–æ–∏: {n_gpu_layers}, –ö–æ–Ω—Ç–µ–∫—Å—Ç: {n_ctx}, –ü–æ—Ç–æ–∫–∏: {n_threads}"
        )

        llm = Llama(
            model_path=model_path,
            n_gpu_layers=n_gpu_layers,
            n_ctx=n_ctx,
            n_threads=n_threads,
            verbose=False,
        )

        logger.info("‚úÖ LLM –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        return llm

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ LLM –º–æ–¥–µ–ª–∏: {e}")
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –ø–æ –æ—à–∏–±–∫–µ
        if "Failed to load model from file" in str(e):
            logger.error(
                "–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã: –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –ø—É—Ç—å/–∏–º—è GGUF, –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π —Ñ–∞–π–ª, –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –∫–≤–∞–Ω—Ç–æ–≤–∫–∞."
            )
            logger.error(
                f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ –∏ –¥–æ—Å—Ç—É–ø: ls -lh {models_dir}; –∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é LLM_MODEL_PATH/KEY."
            )
        raise HTTPException(
            status_code=503, detail=f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å LLM –º–æ–¥–µ–ª—å: {str(e)}"
        )


@lru_cache
def get_query_planner() -> QueryPlannerService:
    settings = get_settings()
    llm = get_llm()
    return QueryPlannerService(llm, settings)


@lru_cache
def get_qa_service() -> QAService:
    settings = get_settings()
    retriever = get_retriever()
    llm = get_llm()
    top_k = int(os.getenv("RETRIEVER_TOP_K", "5"))
    planner = get_query_planner() if settings.enable_query_planner else None
    return QAService(retriever, llm, top_k, settings=settings, planner=planner)


# === –ù–æ–≤—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è ===


@lru_cache
def get_redis_client(settings: Settings = Depends(get_settings)):
    """–°–æ–∑–¥–∞–µ—Ç Redis –∫–ª–∏–µ–Ω—Ç –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ"""
    if not settings.redis_enabled:
        return None

    try:
        import redis

        client = redis.Redis(
            host=settings.redis_host,
            port=settings.redis_port,
            password=settings.redis_password,
            decode_responses=True,
        )
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
        client.ping()
        logger.info(f"Redis –ø–æ–¥–∫–ª—é—á–µ–Ω: {settings.redis_host}:{settings.redis_port}")
        return client
    except Exception as e:
        logger.warning(f"Redis –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        return None
