import os
import logging
from functools import lru_cache
from pathlib import Path
from fastapi import Depends, HTTPException
import chromadb
from llama_cpp import Llama

from adapters.chroma import Retriever
from services.qa_service import QAService
from utils.model_downloader import auto_download_models, RECOMMENDED_MODELS

logger = logging.getLogger(__name__)


@lru_cache
def get_chroma_client():
    """–°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç ChromaDB –∫–ª–∏–µ–Ω—Ç"""
    chroma_path = os.getenv("CHROMA_PATH", "/data/chroma")
    return chromadb.PersistentClient(path=chroma_path)


def get_retriever(client=Depends(get_chroma_client)):
    """–°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç Retriever –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ ChromaDB"""
    collection_name = os.getenv("CHROMA_COLLECTION", "news_demo4")
    embedding_model_key = os.getenv("EMBEDDING_MODEL_KEY", "multilingual-e5-large")

    # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if embedding_model_key in RECOMMENDED_MODELS["embedding"]:
        embedding_model = RECOMMENDED_MODELS["embedding"][embedding_model_key]["name"]
    else:
        # Fallback –Ω–∞ –ø—Ä—è–º–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        embedding_model = os.getenv("EMBEDDING_MODEL", "intfloat/multilingual-e5-large")

    logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º embedding –º–æ–¥–µ–ª—å: {embedding_model}")

    # –ê–≤—Ç–æ—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ embedding –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
    auto_download_embedding = (
        os.getenv("AUTO_DOWNLOAD_EMBEDDING", "true").lower() == "true"
    )
    if auto_download_embedding:
        try:
            from utils.model_downloader import download_embedding_model

            cache_dir = os.getenv("TRANSFORMERS_CACHE", "/models/.cache")
            download_embedding_model(embedding_model, cache_dir)
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å embedding –º–æ–¥–µ–ª—å: {e}")

    return Retriever(client, collection_name, embedding_model)


@lru_cache
def get_llm():
    """–°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç LLM –º–æ–¥–µ–ª—å —Å –∞–≤—Ç–æ—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ–º"""
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    llm_model_key = os.getenv("LLM_MODEL_KEY", "vikhr-7b-instruct")
    models_dir = os.getenv("MODELS_DIR", "/models")
    cache_dir = os.getenv("TRANSFORMERS_CACHE", "/models/.cache")
    auto_download = os.getenv("AUTO_DOWNLOAD_LLM", "true").lower() == "true"

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    n_gpu_layers = int(os.getenv("LLM_GPU_LAYERS", "0"))  # CPU –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    n_ctx = int(os.getenv("LLM_CONTEXT_SIZE", "4096"))
    n_threads = int(os.getenv("LLM_THREADS", "8"))

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
    if llm_model_key in RECOMMENDED_MODELS["llm"]:
        model_config = RECOMMENDED_MODELS["llm"][llm_model_key]
        model_filename = model_config["filename"]
        model_path = os.path.join(models_dir, model_filename)
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –º–æ–¥–µ–ª—å: {model_config['description']}")
    else:
        # Fallback –Ω–∞ –ø—Ä—è–º–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ –ø—É—Ç–∏
        model_path = os.getenv(
            "LLM_MODEL_PATH", f"{models_dir}/Vikhr-7B-instruct-Q4_K_M.gguf"
        )
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏: {model_path}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    if not os.path.exists(model_path):
        if auto_download and llm_model_key in RECOMMENDED_MODELS["llm"]:
            logger.info(f"üîÑ –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∑–∞–ø—É—Å–∫–∞–µ–º –∞–≤—Ç–æ—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ...")

            # –ê–≤—Ç–æ—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
            downloaded_path, _ = auto_download_models(
                llm_model_key=llm_model_key,
                embedding_model_key="",  # –ù–µ —Å–∫–∞—á–∏–≤–∞–µ–º embedding –∑–¥–µ—Å—å
                models_dir=models_dir,
                cache_dir=cache_dir,
            )

            if downloaded_path and os.path.exists(downloaded_path):
                model_path = downloaded_path
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–∞: {model_path}")
            else:
                raise HTTPException(
                    status_code=503,
                    detail=f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å LLM –º–æ–¥–µ–ª—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É.",
                )
        else:
            raise FileNotFoundError(f"LLM model not found at {model_path}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    try:
        logger.info(f"üìö –ó–∞–≥—Ä—É–∂–∞–µ–º LLM –º–æ–¥–µ–ª—å: {os.path.basename(model_path)}")
        logger.info(
            f"   GPU —Å–ª–æ–∏: {n_gpu_layers}, –ö–æ–Ω—Ç–µ–∫—Å—Ç: {n_ctx}, –ü–æ—Ç–æ–∫–∏: {n_threads}"
        )

        llm = Llama(
            model_path=model_path,
            n_gpu_layers=n_gpu_layers,
            n_ctx=n_ctx,
            n_threads=n_threads,
            verbose=False,
        )

        logger.info("‚úÖ LLM –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        return llm

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ LLM –º–æ–¥–µ–ª–∏: {e}")
        raise HTTPException(
            status_code=503, detail=f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å LLM –º–æ–¥–µ–ª—å: {str(e)}"
        )


def get_qa_service(retriever=Depends(get_retriever), llm=Depends(get_llm)):
    """–°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç QA —Å–µ—Ä–≤–∏—Å"""
    top_k = int(os.getenv("RETRIEVER_TOP_K", "5"))
    return QAService(retriever, llm, top_k)
