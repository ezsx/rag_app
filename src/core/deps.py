import os
import logging
from functools import lru_cache
from pathlib import Path
from typing import Optional
from fastapi import Depends, HTTPException
import chromadb
from llama_cpp import Llama
from contextlib import asynccontextmanager

from adapters.chroma import Retriever
from services.qa_service import QAService
from services.query_planner_service import QueryPlannerService
from services.reranker_service import RerankerService
from services.agent_service import AgentService
from utils.model_downloader import auto_download_models, RECOMMENDED_MODELS
from core.settings import get_settings, Settings
from adapters.search.bm25_index import BM25IndexManager
from adapters.search.bm25_retriever import BM25Retriever
from adapters.search.hybrid_retriever import HybridRetriever
from services.tools.tool_runner import ToolRunner

logger = logging.getLogger(__name__)

# –•—Ä–∞–Ω–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–æ–∑–¥–∞–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä LLM, —á—Ç–æ–±—ã –∏–º–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ—Å–≤–æ–±–æ–¥–∏—Ç—å VRAM
_LAST_LLM_INSTANCE: Optional[Llama] = None


@lru_cache
def get_chroma_client():
    """–°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç ChromaDB –∫–ª–∏–µ–Ω—Ç"""
    settings = get_settings()
    try:
        # –ü–æ–ø—Ä–æ–±—É–µ–º HTTP –∫–ª–∏–µ–Ω—Ç (–¥–ª—è Docker compose)
        client = chromadb.HttpClient(
            host=settings.chroma_host, port=settings.chroma_port
        )
        # –ü—Ä–æ–≤–µ—Ä–∏–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
        client.heartbeat()
        logger.info(
            f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ChromaDB HTTP: {settings.chroma_host}:{settings.chroma_port}"
        )
        return client
    except Exception as e:
        logger.warning(f"HTTP –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å ({e}), –ø—Ä–æ–±—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç")
        # Fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç
        return chromadb.PersistentClient(path=settings.chroma_path)


@lru_cache
def get_retriever() -> Retriever:
    settings = get_settings()
    client = get_chroma_client()
    """–°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç Retriever –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ ChromaDB"""
    embedding_model_key = settings.current_embedding_key
    collection_name = settings.current_collection

    # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if embedding_model_key in RECOMMENDED_MODELS["embedding"]:
        embedding_model = RECOMMENDED_MODELS["embedding"][embedding_model_key]["name"]
    else:
        # Fallback –Ω–∞ –ø—Ä—è–º–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        embedding_model = os.getenv("EMBEDDING_MODEL", "intfloat/multilingual-e5-large")

    logger.info(
        f"–ò—Å–ø–æ–ª—å–∑—É–µ–º embedding –º–æ–¥–µ–ª—å: {embedding_model} (–∫–æ–ª–ª–µ–∫—Ü–∏—è: {collection_name})"
    )

    # –ê–≤—Ç–æ—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ embedding –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
    auto_download_embedding = (
        os.getenv("AUTO_DOWNLOAD_EMBEDDING", "true").lower() == "true"
    )
    if auto_download_embedding:
        try:
            from utils.model_downloader import download_embedding_model

            download_embedding_model(embedding_model, settings.cache_dir)
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å embedding –º–æ–¥–µ–ª—å: {e}")

    return Retriever(client, collection_name, embedding_model)


@lru_cache
def get_llm():
    """–°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç LLM –º–æ–¥–µ–ª—å —Å –∞–≤—Ç–æ—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ–º"""
    settings = get_settings()
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
    llm_model_key = settings.current_llm_key
    models_dir = settings.models_dir
    cache_dir = settings.cache_dir
    auto_download = os.getenv("AUTO_DOWNLOAD_LLM", "true").lower() == "true"

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    n_gpu_layers = int(os.getenv("LLM_GPU_LAYERS", "-1"))
    n_ctx = int(os.getenv("LLM_CONTEXT_SIZE", "8192"))
    n_threads = int(os.getenv("LLM_THREADS", "8"))
    n_batch = int(os.getenv("LLM_BATCH", "1024"))

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
    if llm_model_key in RECOMMENDED_MODELS["llm"]:
        model_config = RECOMMENDED_MODELS["llm"][llm_model_key]
        model_filename = model_config["filename"]
        model_path = os.path.join(models_dir, model_filename)
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –º–æ–¥–µ–ª—å: {model_config['description']}")
    else:
        # Fallback –Ω–∞ –ø—Ä—è–º–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ –ø—É—Ç–∏
        model_path = os.getenv("LLM_MODEL_PATH", f"{models_dir}/gpt-oss-20b-Q6_K.gguf")
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏: {model_path}")

    # –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ –ø—É—Ç–∏
    try:
        import llama_cpp as _ll

        logger.info(f"llama_cpp version: {getattr(_ll, '__version__', 'unknown')}")
    except Exception as _:
        logger.info("llama_cpp version: <unavailable>")
    logger.info(
        f"ENV: CUDA_VISIBLE_DEVICES={os.getenv('CUDA_VISIBLE_DEVICES')}, "
        f"LLM_GPU_LAYERS={os.getenv('LLM_GPU_LAYERS', str(n_gpu_layers))}, "
        f"LLM_CONTEXT_SIZE={os.getenv('LLM_CONTEXT_SIZE', str(n_ctx))}, "
        f"LLM_THREADS={os.getenv('LLM_THREADS', str(n_threads))}, "
        f"LLM_BATCH={os.getenv('LLM_BATCH', str(n_batch))}"
    )
    logger.info(
        f"ENV: LLM_MODEL_PATH={os.getenv('LLM_MODEL_PATH')}, MODELS_DIR={models_dir}, PWD={os.getcwd()}"
    )
    logger.info(f"–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏: {model_path}")
    if os.path.exists(model_path):
        try:
            sz = Path(model_path).stat().st_size
            logger.info(f"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏: {sz / (1024**3):.2f} GB")
        except Exception:
            pass
        # –ü–µ—Ä–µ—á–∏—Å–ª–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ –≤ –∫–∞—Ç–∞–ª–æ–≥–µ /models –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ volume
        try:
            from itertools import islice

            entries = list(islice(Path(models_dir).glob("*"), 10))
            sample = ", ".join([e.name for e in entries])
            logger.info(f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ /models (–ø–µ—Ä–≤—ã–µ 10): {sample}")
        except Exception as _e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ /models: {_e}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø –∏ –º–∞–≥–∏—á–µ—Å–∫–æ–µ —á–∏—Å–ª–æ GGUF + –≤–µ—Ä—Å–∏—é
        try:
            r_ok = os.access(model_path, os.R_OK)
            logger.info(f"–î–æ—Å—Ç—É–ø –∫ –º–æ–¥–µ–ª–∏ (readable): {r_ok}")
        except Exception:
            pass
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞–≥–∏—á–µ—Å–∫–æ–µ —á–∏—Å–ª–æ GGUF
        try:
            with open(model_path, "rb") as f:
                magic = f.read(4)
                version_bytes = f.read(4)
            try:
                import struct

                version = struct.unpack("<I", version_bytes)[0]
            except Exception:
                version = -1
            logger.info(f"GGUF header: {magic!r}, version={version}")
        except Exception as _e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ GGUF: {_e}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    if not os.path.exists(model_path):
        if auto_download and llm_model_key in RECOMMENDED_MODELS["llm"]:
            logger.info(f"üîÑ –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∑–∞–ø—É—Å–∫–∞–µ–º –∞–≤—Ç–æ—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ...")

            # –ê–≤—Ç–æ—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
            downloaded_path, _ = auto_download_models(
                llm_model_key=llm_model_key,
                embedding_model_key="",  # –ù–µ —Å–∫–∞—á–∏–≤–∞–µ–º embedding –∑–¥–µ—Å—å
                models_dir=models_dir,
                cache_dir=cache_dir,
            )

            if downloaded_path and os.path.exists(downloaded_path):
                model_path = downloaded_path
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–∞: {model_path}")
            else:
                raise HTTPException(
                    status_code=503,
                    detail=f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å LLM –º–æ–¥–µ–ª—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É.",
                )
        else:
            raise FileNotFoundError(f"LLM model not found at {model_path}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    try:
        # –í–∫–ª—é—á–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω—ã–π –ª–æ–≥ backend –∏ –±—ã—Å—Ç—Ä—ã–π CUDA-–±—ç–∫–µ–Ω–¥ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        os.environ.setdefault("LLAMA_LOG_LEVEL", "DEBUG")
        os.environ.setdefault("GGML_VERBOSE", "1")
        os.environ.setdefault("GGML_CUDA_FORCE_CUBLAS", "1")
        logger.info(f"üìö –ó–∞–≥—Ä—É–∂–∞–µ–º LLM –º–æ–¥–µ–ª—å: {os.path.basename(model_path)}")
        logger.info(
            f"   GPU —Å–ª–æ–∏: {n_gpu_layers}, –ö–æ–Ω—Ç–µ–∫—Å—Ç: {n_ctx}, –ü–æ—Ç–æ–∫–∏: {n_threads}, –ë–∞—Ç—á: {n_batch}"
        )

        try:
            llm = Llama(
                model_path=model_path,
                n_gpu_layers=n_gpu_layers,
                n_ctx=n_ctx,
                n_threads=n_threads,
                n_batch=n_batch,
                verbose=True,
            )
        except Exception as e1:
            logger.error(f"–ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ Llama –Ω–µ —É–¥–∞–ª—Å—è: {e1}. –ü—Ä–æ–±—É–µ–º use_mmap=False")
            try:
                llm = Llama(
                    model_path=model_path,
                    n_gpu_layers=n_gpu_layers,
                    n_ctx=n_ctx,
                    n_threads=n_threads,
                    n_batch=n_batch,
                    use_mmap=False,
                    verbose=True,
                )
            except Exception as e2:
                logger.error(
                    f"–ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—É—Å–∫ Llama –Ω–µ —É–¥–∞–ª—Å—è: {e2}. –ü—Ä–æ–±—É–µ–º use_mlock=False"
                )
                llm = Llama(
                    model_path=model_path,
                    n_gpu_layers=n_gpu_layers,
                    n_ctx=n_ctx,
                    n_threads=n_threads,
                    n_batch=n_batch,
                    use_mmap=False,
                    use_mlock=False,
                    verbose=True,
                )

        logger.info("‚úÖ LLM –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–æ–∑–¥–∞–Ω–Ω—ã–π LLM
        global _LAST_LLM_INSTANCE
        _LAST_LLM_INSTANCE = llm
        return llm

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ LLM –º–æ–¥–µ–ª–∏: {e}")
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –ø–æ –æ—à–∏–±–∫–µ
        if "Failed to load model from file" in str(e):
            logger.error(
                "–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã: –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –ø—É—Ç—å/–∏–º—è GGUF, –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π —Ñ–∞–π–ª, –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –∫–≤–∞–Ω—Ç–æ–≤–∫–∞."
            )
            logger.error(
                f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ –∏ –¥–æ—Å—Ç—É–ø: ls -lh {models_dir}; –∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é LLM_MODEL_PATH/KEY."
            )
        raise HTTPException(
            status_code=503, detail=f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å LLM –º–æ–¥–µ–ª—å: {str(e)}"
        )


# –í—Ä–µ–º–µ–Ω–Ω–æ–µ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ VRAM LLM –¥–ª—è ingestion (–ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –∑–∞–¥–∞—á –∏–Ω–∂–µ—Å—Ç–∞)
@asynccontextmanager
async def release_llm_vram_temporarily():
    """
    –û—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç VRAM, –∑–∞–Ω—è—Ç—É—é LLM (llama-cpp), –Ω–∞ –≤—Ä–µ–º—è –±–ª–æ–∫–∞.
    –ü–æ–¥—Ö–æ–¥: –Ω–∞ –ª–µ—Ç—É –≤—ã–≥—Ä—É–∂–∞–µ–º GPU-—Å–ª–æ–∏, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è n_gpu_layers=0 —á–µ—Ä–µ–∑ recreate;
    –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ, –ø—ã—Ç–∞–µ–º—Å—è –≤—ã–∑–≤–∞—Ç—å low-level free_buffers/flush_kv_cache.
    –ü–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞—Ç–Ω–æ (—á—Ç–æ–±—ã –Ω–µ –∑–∞–Ω–æ–≤–æ –∑–∞–Ω–∏–º–∞—Ç—å VRAM).
    –í —Å—Ü–µ–Ω–∞—Ä–∏–∏ QA —Å–µ—Ä–≤–∏—Å –ø–æ–≤—Ç–æ—Ä–Ω–æ —Å–æ–∑–¥–∞—Å—Ç LLM –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –∑–∞–ø—Ä–æ—Å–µ —á–µ—Ä–µ–∑ lru_cache.clear.
    """
    # –í–ê–ñ–ù–û: –Ω–µ –≤—ã–∑—ã–≤–∞–µ–º get_llm() –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –Ω–µ –∏–Ω–∏—Ü–∏–∏—Ä–æ–≤–∞—Ç—å –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏.
    try:
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ –±—ã–ª–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º –æ—Å–≤–æ–±–æ–¥–∏—Ç—å —Ä–µ—Å—É—Ä—Å—ã
        global _LAST_LLM_INSTANCE
        llm = _LAST_LLM_INSTANCE
        if llm is not None:
            try:
                if hasattr(llm, "flush_kv_cache"):
                    llm.flush_kv_cache()
            except Exception:
                pass
            # –ü–æ–ø—Ä–æ–±—É–µ–º –∏–Ω–∏—Ü–∏–∏—Ä–æ–≤–∞—Ç—å GC —á–µ—Ä–µ–∑ –¥–µ—Å—Ç—Ä—É–∫—Ç–æ—Ä
            try:
                if hasattr(llm, "__del__"):
                    llm.__del__()
            except Exception:
                pass
        # –°–±—Ä–æ—Å–∏–º –∫—ç—à —Ñ–∞–±—Ä–∏–∫–∏ LLM, —á—Ç–æ–±—ã –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º QA-–∑–∞–ø—Ä–æ—Å–µ –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–≤–∞–ª–∞—Å—å –∑–∞–Ω–æ–≤–æ
        try:
            get_llm.cache_clear()  # type: ignore[attr-defined]
        except Exception:
            pass
        # –û–±–Ω—É–ª–∏–º –Ω–∞—à—É —Å—Å—ã–ª–∫—É ‚Äî –¥–∞—ë–º GC —à–∞–Ω—Å –æ—Å–≤–æ–±–æ–¥–∏—Ç—å VRAM
        try:
            _LAST_LLM_INSTANCE = None
        except Exception:
            pass
        yield
    finally:
        # –ù–µ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏; LLM –ø–æ–¥–Ω–∏–º–µ—Ç—Å—è –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º QA-–∑–∞–ø—Ä–æ—Å–µ
        pass


@lru_cache
def get_query_planner() -> QueryPlannerService:
    settings = get_settings()
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—É—é LLM –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ (CPU), –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞
    try:
        planner_llm = get_planner_llm()
    except Exception:
        planner_llm = get_llm()
    return QueryPlannerService(planner_llm, settings)


@lru_cache
def get_planner_llm():
    """–°–æ–∑–¥–∞–µ—Ç LLM –¥–ª—è Query Planner (–∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, CPU Qwen2.5-3B-Instruct)."""
    import os
    from llama_cpp import Llama

    settings = get_settings()
    key = settings.planner_llm_key
    models_dir = settings.models_dir
    cache_dir = settings.cache_dir

    # –ê–≤—Ç–æ—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ, –µ—Å–ª–∏ –≤ —Ä–µ–µ—Å—Ç—Ä–µ
    try:
        if key in RECOMMENDED_MODELS["llm"]:
            logger.info(
                f"üì• Planner LLM: {RECOMMENDED_MODELS['llm'][key]['description']}"
            )
            from utils.model_downloader import auto_download_models

            path, _ = auto_download_models(
                llm_model_key=key,
                embedding_model_key="",
                models_dir=models_dir,
                cache_dir=cache_dir,
            )
            model_path = path or os.getenv(
                "PLANNER_LLM_MODEL_PATH",
                f"{models_dir}/{RECOMMENDED_MODELS['llm'][key]['filename']}",
            )
        else:
            model_path = os.getenv("PLANNER_LLM_MODEL_PATH", f"{models_dir}/{key}.gguf")
    except Exception as e:
        logger.warning(f"Planner LLM auto-download skipped: {e}")
        model_path = os.getenv("PLANNER_LLM_MODEL_PATH", f"{models_dir}/{key}.gguf")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ CPU (–±–µ–∑ VRAM). –î–ª—è Qwen2.5 –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π chat —Ñ–æ—Ä–º–∞—Ç.
    chat_format = os.getenv("PLANNER_CHAT_FORMAT", "auto")
    kwargs = dict(
        model_path=model_path,
        n_gpu_layers=0,
        n_ctx=int(os.getenv("PLANNER_LLM_CONTEXT_SIZE", "2048")),
        n_threads=int(os.getenv("PLANNER_LLM_THREADS", "6")),
        n_batch=int(os.getenv("PLANNER_LLM_BATCH", "256")),
        verbose=False,
    )
    # –ï—Å–ª–∏ —è–≤–Ω–æ —É–∫–∞–∑–∞–ª–∏ —Ñ–æ—Ä–º–∞—Ç ‚Äî –ø—Ä–æ–±—Ä–æ—Å–∏–º, –∏–Ω–∞—á–µ –æ—Å—Ç–∞–≤–∏–º auto (llama.cpp –∏–∑ GGUF)
    if chat_format and chat_format != "auto":
        kwargs["chat_format"] = chat_format

    llm = Llama(**kwargs)  # type: ignore[arg-type]
    logger.info(f"‚úÖ Planner LLM –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ CPU: {os.path.basename(model_path)}")
    return llm


@lru_cache
def get_bm25_index_manager() -> BM25IndexManager:
    settings = get_settings()
    return BM25IndexManager(
        index_root=settings.bm25_index_root,
        reload_min_interval_sec=settings.bm25_reload_min_interval_sec,
    )


@lru_cache
def get_qa_service() -> QAService:
    settings = get_settings()
    retriever = get_retriever()

    # –ü–µ—Ä–µ–¥–∞–µ–º —Ñ–∞–±—Ä–∏–∫—É –≤–º–µ—Å—Ç–æ –≥–æ—Ç–æ–≤–æ–π LLM: –ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ
    def _llm_factory():
        return get_llm()

    top_k = int(os.getenv("RETRIEVER_TOP_K", "5"))
    planner = get_query_planner() if settings.enable_query_planner else None
    reranker = get_reranker() if settings.enable_reranker else None

    hybrid = None
    if settings.hybrid_enabled:
        try:
            bm25_mgr = get_bm25_index_manager()
            bm25_ret = BM25Retriever(bm25_mgr, settings)
            hybrid = HybridRetriever(bm25_ret, retriever, settings)
        except Exception as e:
            logger.warning(f"Hybrid retriever init failed: {e}")

    return QAService(
        retriever,
        _llm_factory,
        top_k,
        settings=settings,
        planner=planner,
        reranker=reranker,
        hybrid=hybrid,
    )


@lru_cache
def get_hybrid_retriever() -> Optional[HybridRetriever]:
    settings = get_settings()
    if not settings.hybrid_enabled:
        return None
    try:
        bm25_mgr = get_bm25_index_manager()
        dense_ret = get_retriever()
        bm25_ret = BM25Retriever(bm25_mgr, settings)
        return HybridRetriever(bm25_ret, dense_ret, settings)
    except Exception as e:
        logger.warning(f"Hybrid retriever init failed: {e}")
        return None


@lru_cache
def get_reranker() -> Optional[RerankerService]:
    settings = get_settings()
    if not settings.enable_reranker:
        return None
    # –ê–≤—Ç–æ—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ä–µ—Ä–µ–π–∫–µ—Ä–∞ (–∫–∞–∫ –∏ –¥–ª—è LLM/Embedding)
    auto_download_reranker = (
        os.getenv("AUTO_DOWNLOAD_RERANKER", "true").lower() == "true"
    )
    if auto_download_reranker:
        try:
            from utils.model_downloader import download_reranker_model

            download_reranker_model(settings.reranker_model_key, settings.cache_dir)
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å reranker –º–æ–¥–µ–ª—å –∑–∞—Ä–∞–Ω–µ–µ: {e}")
    return RerankerService(settings.reranker_model_key)


# === –ù–æ–≤—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è ===


@lru_cache
def get_redis_client(settings: Settings = Depends(get_settings)):
    """–°–æ–∑–¥–∞–µ—Ç Redis –∫–ª–∏–µ–Ω—Ç –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ"""
    if not settings.redis_enabled:
        return None

    try:
        import redis

        client = redis.Redis(
            host=settings.redis_host,
            port=settings.redis_port,
            password=settings.redis_password,
            decode_responses=True,
        )
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
        client.ping()
        logger.info(f"Redis –ø–æ–¥–∫–ª—é—á–µ–Ω: {settings.redis_host}:{settings.redis_port}")
        return client
    except Exception as e:
        logger.warning(f"Redis –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        return None


@lru_cache
def get_agent_service() -> AgentService:
    """–°–æ–∑–¥–∞–µ—Ç AgentService —Å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏"""
    settings = get_settings()

    # –°–æ–∑–¥–∞–µ–º ToolRunner —Å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º —Ç–∞–π–º–∞—É—Ç–æ–º
    tool_runner = ToolRunner(default_timeout_sec=settings.agent_tool_timeout)

    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
    from services.tools.router_select import router_select
    from services.tools.compose_context import compose_context
    from services.tools.fetch_docs import fetch_docs
    from services.tools.verify import verify
    from services.tools.search import search
    from services.tools.query_plan import query_plan
    from services.tools.rerank import rerank

    # –ü–æ–ª—É—á–∞–µ–º retriever –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –µ–≥–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç
    retriever = get_retriever()

    # –ü–æ–ª—É—á–∞–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π retriever –¥–ª—è search –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
    from adapters.search.hybrid_retriever import HybridRetriever

    # –ü—ã—Ç–∞–µ–º—Å—è –≤–∑—è—Ç—å –≥–æ—Ç–æ–≤—ã–π –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä –∏–∑ –∫–µ—à–∞
    hybrid_retriever = get_hybrid_retriever()
    if hybrid_retriever is None:
        # –§–æ–ª–±—ç–∫: —Å–æ–±–∏—Ä–∞–µ–º –∏–∑ BM25 –∏ dense –≤—Ä—É—á–Ω—É—é
        try:
            bm25_mgr = get_bm25_index_manager()
            bm25_ret = BM25Retriever(bm25_mgr, settings)
            dense_ret = retriever  # —Ç–µ–∫—É—â–∏–π dense Retriever –∏–∑ Chroma
            hybrid_retriever = HybridRetriever(bm25_ret, dense_ret, settings)
        except Exception as e:
            logger.warning(f"Hybrid retriever fallback init failed: {e}")
            hybrid_retriever = None

    # –ü–æ–ª—É—á–∞–µ–º reranker –¥–ª—è rerank –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
    from services.reranker_service import RerankerService

    reranker = get_reranker()

    # –ü–æ–ª—É—á–∞–µ–º query planner –¥–ª—è query_plan –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
    from services.query_planner_service import QueryPlannerService

    def _llm_factory():
        return get_llm()

    query_planner = get_query_planner() if settings.enable_query_planner else None

    # –°–æ–∑–¥–∞–µ–º –æ–±–µ—Ä—Ç–∫–∏ –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–¥–∞—é—Ç—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è—Ö
    def verify_wrapper(**kwargs):
        return verify(retriever=retriever, **kwargs)

    def fetch_docs_wrapper(**kwargs):
        return fetch_docs(retriever=retriever, **kwargs)

    # –≠–∫—Å–ø–ª–∏—Ü–∏—Ç–Ω—ã–µ –∑–∞–≤–∏–º–æ—Å–∏—Ç–∏ –¥–ª—è search-—Ñ–æ–ª–±—ç–∫–æ–≤
    bm25_mgr = get_bm25_index_manager()
    bm25_ret = BM25Retriever(bm25_mgr, settings)

    def search_wrapper(**kwargs):
        return search(
            hybrid_retriever=hybrid_retriever,
            bm25_retriever=bm25_ret,
            **kwargs,
        )

    def query_plan_wrapper(**kwargs):
        return query_plan(query_planner=query_planner, **kwargs)

    def rerank_wrapper(**kwargs):
        return rerank(reranker=reranker, **kwargs)

    tool_runner.register("router_select", router_select)
    tool_runner.register("query_plan", query_plan_wrapper, timeout_sec=6.0)
    tool_runner.register(
        "search",
        search_wrapper,
        timeout_sec=max(5.0, settings.agent_tool_timeout * 0.8),
    )
    tool_runner.register(
        "rerank",
        rerank_wrapper,
        timeout_sec=max(4.0, settings.agent_tool_timeout * 0.5),
    )
    tool_runner.register("fetch_docs", fetch_docs_wrapper)
    tool_runner.register("compose_context", compose_context)
    tool_runner.register("verify", verify_wrapper)

    # –ü–µ—Ä–µ–¥–∞–µ–º —Ñ–∞–±—Ä–∏–∫—É LLM –¥–ª—è –ª–µ–Ω–∏–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
    def _llm_factory():
        return get_llm()

    # –ü–æ–ª—É—á–∞–µ–º QA —Å–µ—Ä–≤–∏—Å –¥–ª—è fallback
    qa_service = get_qa_service()

    return AgentService(
        llm_factory=_llm_factory,
        tool_runner=tool_runner,
        settings=settings,
        qa_service=qa_service,
    )
