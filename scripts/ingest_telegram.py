"""
Telegram ‚Üí ChromaDB ingestor
----------------------------
CLI‚Äë—Å–∫—Ä–∏–ø—Ç, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –ø—É–±–ª–∏—á–Ω–æ–≥–æ/–ø—Ä–∏–≤–∞—Ç–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞ Telegram
–≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ –¥–∞—Ç, —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è (BGE‚Äëbase‚Äëv1.5 –∏–ª–∏ –¥—Ä—É–≥–∞—è
—É–∫–∞–∑–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å) –∏ –ø–∞—á–∫–∞–º–∏ –ø–∏—à–µ—Ç –∏—Ö –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é ChromaDB.

–ó–∞–ø—É—Å–∫ (–ø—Ä–∏–º–µ—Ä):
$ python -m scripts.ingest_telegram \
      --channel @some_channel \
      --since 2024-06-01 \
      --until 2024-07-01 \
      --collection tg_some_channel

–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø–æ–ª–æ–∂–∏—Ç–µ –≤ .env:
TG_API_ID=123456
TG_API_HASH=abcdef0123456789abcdef0123456789
# –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ user‚Äëaccount –ª–æ–≥–∏–Ω ‚Üí TG_PHONE=+79995555555
# –ª–∏–±–æ BOT_TOKEN=123456:ABC‚ÄëDEF‚Ä¶ (—Ç–æ–≥–¥–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –±–æ—Ç—É)

CHROMA_HOST=localhost
CHROMA_PORT=8000

–°–∫—Ä–∏–ø—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç —Ä–∞–±–æ—Ç—É –ø–æ—Å–ª–µ –æ–±—Ä—ã–≤–∞ —Å–µ—Ç–∏ –∏–ª–∏ FloodWait,
–ø–∏—à–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –∏ –æ–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è.
"""

from __future__ import annotations

import asyncio
import os
import sys
import uuid
import logging
import argparse
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any

from dotenv import load_dotenv
from tqdm.asyncio import tqdm_asyncio
from dateutil import parser as date_parser, tz

from telethon import TelegramClient, errors as tg_errors, events
from telethon.errors import FloodWaitError, SessionPasswordNeededError
from telethon.tl.types import Message

import numpy as np 
import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
import torch
from sentence_transformers import SentenceTransformer

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Config & CLI parsing
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def _parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Ingest Telegram channel into ChromaDB")
    p.add_argument("--channel", required=True, help="@username or chat id")
    p.add_argument("--since", required=True, help="ISO date (inclusive)")
    p.add_argument("--until", required=True, help="ISO date (exclusive)")
    p.add_argument("--collection", required=True, help="Chroma collection name")
    p.add_argument(
        "--batch-size",
        type=int,
        default=None,
        help="Batch size (auto-detected based on device)",
    )
    p.add_argument(
        "--embed-model-key", default=None, help="Embedding model key from config"
    )
    p.add_argument(
        "--embed-model", default=None, help="Direct embedding model name (fallback)"
    )
    p.add_argument("--device", default="auto", help="Device: auto, cpu, cuda, mps")
    p.add_argument(
        "--max-messages",
        type=int,
        default=None,
        help="ingest N newest messages then stop (debug)",
    )
    p.add_argument("--log-level", default="INFO")
    p.add_argument(
        "--gpu-batch-multiplier", type=int, default=4, help="GPU batch size multiplier"
    )
    return p.parse_args()


load_dotenv()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Logger
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
logger = logging.getLogger("ingest_telegram")
handler = logging.StreamHandler(sys.stdout)
handler.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s"))
logger.addHandler(handler)
logger.setLevel(os.getenv("LOG_LEVEL", "INFO"))

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Telegram utils
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


async def create_telegram_client() -> TelegramClient:
    api_id_str = os.getenv("TG_API_ID")
    if not api_id_str:
        logger.error("TG_API_ID not set; add it to .env or environment")
        sys.exit(1)
    api_id = int(api_id_str)
    api_hash = os.getenv("TG_API_HASH")
    # Persist session file to avoid re-auth every run. Using TG_SESSION allows
    # passing an absolute path (e.g. a mounted Docker volume). If the path
    # points to a directory, we append the default filename so Telethon gets a
    # valid file path.
    session_path_env = os.getenv("TG_SESSION", "telegram.session")
    # Expand tilde and make absolute for safety
    session_path = os.path.expanduser(session_path_env)
    if os.path.isdir(session_path):
        session_path = os.path.join(session_path, "telegram.session")

    # Ensure parent dir exists so Telethon can write the session file even in a
    # freshly mounted volume.
    parent_dir = os.path.dirname(session_path)
    if parent_dir:
        os.makedirs(parent_dir, exist_ok=True)

    phone = os.getenv("TG_PHONE")
    bot_token = os.getenv("BOT_TOKEN")

    if not (api_id and api_hash):
        logger.error("TG_API_ID and TG_API_HASH must be set in environment")
        sys.exit(1)

    client = TelegramClient(session_path, api_id, api_hash, device_model="RAG-Scraper")
    await client.connect()

    if not await client.is_user_authorized():
        if bot_token:
            await client.start(bot_token=bot_token)
        elif phone:
            await client.send_code_request(phone)
            code = input("Enter the code you just received: ")
            try:
                await client.sign_in(phone, code)
            except SessionPasswordNeededError:
                pwd = input("Two‚Äëfactor enabled. Enter your password: ")
                await client.sign_in(password=pwd)
        else:
            logger.error("Provide BOT_TOKEN or TG_PHONE for authorization.")
            sys.exit(1)
    return client


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Chroma utils
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def detect_optimal_device() -> str:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ª—É—á—à–µ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π"""
    if torch.cuda.is_available():
        logger.info(f"üöÄ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.get_device_name(0)}")
        return "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        logger.info("üöÄ MPS (Apple Silicon) –¥–æ—Å—Ç—É–ø–µ–Ω")
        return "mps"
    else:
        logger.info("üíª –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU")
        return "cpu"


def get_optimal_batch_size(
    device: str, base_batch_size: int = 64, gpu_multiplier: int = 4
) -> int:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
    if device in ["cuda", "mps"]:
        optimal_size = base_batch_size * gpu_multiplier
        logger.info(f"üìä GPU detected, increasing batch size to {optimal_size}")
        return optimal_size
    else:
        logger.info(f"üìä CPU detected, using batch size {base_batch_size}")
        return base_batch_size


def resolve_embedding_model(
    embed_model_key: Optional[str], embed_model: Optional[str]
) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º–æ–¥–µ–ª—å embedding –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–ª–∏ fallback"""
    # –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ .env –∫–∞–∫ –≤ API
    if not embed_model_key:
        embed_model_key = os.getenv("EMBEDDING_MODEL_KEY", "multilingual-e5-large")

    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π
    try:
        import sys
        from pathlib import Path

        # –î–æ–±–∞–≤–ª—è–µ–º src –≤ –ø—É—Ç—å
        src_path = Path(__file__).parent.parent / "src"
        if str(src_path) not in sys.path:
            sys.path.insert(0, str(src_path))

        from utils.model_downloader import RECOMMENDED_MODELS

        if embed_model_key in RECOMMENDED_MODELS["embedding"]:
            model_name = RECOMMENDED_MODELS["embedding"][embed_model_key]["name"]
            logger.info(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {model_name}")
            return model_name
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π: {e}")

    # Fallback
    if embed_model:
        logger.info(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∫–∞–∑–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å: {embed_model}")
        return embed_model

    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
    default_model = os.getenv("EMBEDDING_MODEL", "intfloat/multilingual-e5-large")
    logger.info(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {default_model}")
    return default_model


class FastEmbeddingFunction:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è embedding —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GPU –∏ –±–∞—Ç—á–∏–Ω–≥–∞"""

    def __init__(self, model_name: str, device: str):
        self.model_name = model_name
        self.device = device
        logger.info(f"‚ö° –ó–∞–≥—Ä—É–∂–∞–µ–º embedding –º–æ–¥–µ–ª—å: {model_name} –Ω–∞ {device}")
        self.model = SentenceTransformer(model_name, device=device)
        if device in ["cuda", "mps"]:
            try:
                self.model.half()
            except AttributeError:
                pass
            logger.info("üî• –í–∫–ª—é—á–µ–Ω–∞ GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è")

    # sig must be (self, input)
    def __call__(self, input):
        if not input:
            return np.empty((0, 768), dtype=np.float32)
        with torch.no_grad():
            emb = self.model.encode(
                input,
                convert_to_tensor=False,          # np.ndarray
                device=self.device,
                show_progress_bar=False,
                batch_size=32 if self.device == "cpu" else 128,
            )
            return emb



def create_chroma_collection(name: str, embed_model: str, device: str):
    """Connect to Chroma HTTP server and obtain (or create) a collection.

    The Chroma Python API changed multiple times; we probe the available
    symbols/parameters at runtime so that the script works with 0.4.x and
    newer versions alike."""

    # ---------------------------
    # Build embedding function
    # ---------------------------
    from inspect import signature

    hf_kwargs: Dict[str, Any] = {"model_name": embed_model}

    sig = signature(SentenceTransformerEmbeddingFunction)

    if "device" in sig.parameters:
        hf_kwargs["device"] = device
    elif "device_type" in sig.parameters:
        hf_kwargs["device_type"] = device

    # Optional normalization flag changed name across versions.
    if "normalize" in sig.parameters:
        hf_kwargs["normalize"] = True
    elif "normalize_embeddings" in sig.parameters:
        hf_kwargs["normalize_embeddings"] = True

    # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á HuggingFace –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω,
    # —Ç–∞–∫ –∫–∞–∫ –Ω–æ–≤—ã–µ –≤–µ—Ä—Å–∏–∏ chromadb —Ç—Ä–µ–±—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–≥–æ `api_key`.
    api_key = os.getenv("CHROMA_HUGGINGFACE_API_KEY") or os.getenv(
        "HUGGINGFACE_API_KEY"
    )
    if api_key:
        if "api_key" in sig.parameters:
            hf_kwargs["api_key"] = api_key
        elif "huggingface_api_key" in sig.parameters:
            hf_kwargs["huggingface_api_key"] = api_key

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—à—É –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é embedding
    try:
        embed_fn = FastEmbeddingFunction(embed_model, device)
        logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é embedding —Ñ—É–Ω–∫—Ü–∏—é")
    except Exception as e:
        logger.warning(f"Fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é embedding —Ñ—É–Ω–∫—Ü–∏—é: {e}")
        embed_fn = SentenceTransformerEmbeddingFunction(**hf_kwargs)

    # ---------------------------
    # Connect to Chroma
    # ---------------------------
    host = os.getenv("CHROMA_HOST", "localhost")
    port = int(os.getenv("CHROMA_PORT", 8000))

    if hasattr(chromadb, "HttpClient"):
        client = chromadb.HttpClient(host=host, port=port)
    else:
        # Fallback to old API
        client = chromadb.Client(host=host, port=port)

    # ---------------------------
    # Get or create collection (with embedding function attached)
    # ---------------------------
    try:
        collection = client.get_collection(name, embedding_function=embed_fn)
        logger.info("Using existing collection %s (count=%s)", name, collection.count())
    except Exception as e:
        # get_collection may raise NotFoundError (old API) or ValueError (newer)
        logger.debug("Collection %s not found (%s), creating new", name, e)
        collection = client.create_collection(
            name,
            metadata={"hnsw:space": "cosine"},
            embedding_function=embed_fn,
        )
        logger.info("Created new collection %s", name)

    return collection


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Batch helpers
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def split_into_batches(seq: List[Any], batch_size: int):
    for i in range(0, len(seq), batch_size):
        yield seq[i : i + batch_size]


def estimate_processing_time(total_messages: int, batch_size: int, device: str) -> str:
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
    # –ü—Ä–∏–º–µ—Ä–Ω—ã–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ (—Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Å–µ–∫—É–Ω–¥—É)
    speeds = {
        "cuda": 100,  # ~100 —Å–æ–æ–±—â–µ–Ω–∏–π/—Å–µ–∫ –Ω–∞ GPU
        "mps": 80,  # ~80 —Å–æ–æ–±—â–µ–Ω–∏–π/—Å–µ–∫ –Ω–∞ Apple Silicon
        "cpu": 5,  # ~5 —Å–æ–æ–±—â–µ–Ω–∏–π/—Å–µ–∫ –Ω–∞ CPU
    }

    speed = speeds.get(device, 5)
    estimated_seconds = total_messages / speed

    if estimated_seconds < 60:
        return f"{estimated_seconds:.0f} —Å–µ–∫—É–Ω–¥"
    elif estimated_seconds < 3600:
        return f"{estimated_seconds/60:.1f} –º–∏–Ω—É—Ç"
    else:
        return f"{estimated_seconds/3600:.1f} —á–∞—Å–æ–≤"


async def gather_messages(
    client: TelegramClient,
    channel: str,
    start: datetime,
    end: datetime,
    limit: Optional[int] = None,
) -> List[Message]:
    """Fetch messages with *start* ‚â§ date < *end*.

    Telegram's API returns history in reverse-chronological order by default
    (newest ‚Üí oldest). We leverage that to avoid walking the whole history of
    busy channels: request messages older than *end* and stop once we cross
    *start*."""

    collected: List[Message] = []

    # Request messages older-than `end` (exclusive). We keep the default
    # order (newest‚Üíoldest) so once we reach dates older than `start` we can
    # break early.
    async for msg in client.iter_messages(channel, offset_date=end):
        # Message dates from Telethon are timezone-aware UTC; normalise for
        # reliable comparisons.
        msg_dt = _to_utc_naive(msg.date)

        if msg_dt < start:
            break  # we went past the desired range
        if msg_dt >= end:
            continue  # should not normally happen, but be safe

        if not (msg.message and msg.message.strip()):
            continue  # skip non-text and service messages

        collected.append(msg)

        if limit and len(collected) >= limit:
            break

    # We collected newest‚Üíoldest; reverse to chronological order for nicer
    # batching/ingestion.
    collected.reverse()
    return collected


async def ingest_batches(collection, messages: List[Message], batch_size: int):
    total = len(messages)
    for batch in tqdm_asyncio(
        split_into_batches(messages, batch_size),
        total=(total // batch_size) + 1,
        desc="Ingesting",
        unit="batch",
    ):
        docs = [m.message for m in batch]
        ids = [f"{m.id}_{uuid.uuid4().hex[:6]}" for m in batch]
        metas = []
        for m in batch:
            meta = {
                "channel_id": m.chat_id,
                "msg_id": m.id,
                "date": m.date.isoformat(),
            }
            if m.reply_to_msg_id is not None:
                meta["reply_to"] = m.reply_to_msg_id
            views = getattr(m, "views", None)
            if views is not None:
                meta["views"] = views
            metas.append(meta)
        try:
            collection.add(documents=docs, metadatas=metas, ids=ids)
        except Exception as e:
            logger.exception("Failed on batch, skipping‚Ä¶ (%s)", e)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Date helpers
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def _to_utc_naive(dt: datetime) -> datetime:
    """Return timezone-naive UTC datetime for reliable comparison with
    Telegram timestamps (which Telethon returns as naive UTC)."""
    if dt.tzinfo is None:
        return dt
    return dt.astimezone(timezone.utc).replace(tzinfo=None)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Main
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


async def main():
    args = _parse_args()
    logger.setLevel(args.log_level.upper())

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    if args.device == "auto":
        device = detect_optimal_device()
    else:
        device = args.device
        logger.info(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∫–∞–∑–∞–Ω–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å embedding
    embed_model = resolve_embedding_model(args.embed_model_key, args.embed_model)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
    if args.batch_size is None:
        batch_size = get_optimal_batch_size(
            device, gpu_multiplier=args.gpu_batch_multiplier
        )
    else:
        batch_size = args.batch_size
        logger.info(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∫–∞–∑–∞–Ω–Ω—ã–π batch size: {batch_size}")

    start_iso = _to_utc_naive(date_parser.isoparse(args.since))
    end_iso = _to_utc_naive(date_parser.isoparse(args.until))

    logger.info("üîó –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Telegram‚Ä¶")
    client = await create_telegram_client()
    try:
        logger.info("üì• –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π %s ‚Üí %s", start_iso.date(), end_iso.date())
        msgs = await gather_messages(
            client, args.channel, start_iso, end_iso, args.max_messages
        )
        logger.info("üì® –ü–æ–ª—É—á–µ–Ω–æ %s —Å–æ–æ–±—â–µ–Ω–∏–π", len(msgs))

        if not msgs:
            logger.warning("‚ùå –°–æ–æ–±—â–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã ‚Äî –∑–∞–≤–µ—Ä—à–∞–µ–º")
            return

        # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        estimated_time = estimate_processing_time(len(msgs), batch_size, device)
        logger.info(f"‚è±Ô∏è –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {estimated_time}")

        logger.info(
            "üîó –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ChromaDB %s:%s",
            os.getenv("CHROMA_HOST", "localhost"),
            os.getenv("CHROMA_PORT", 8000),
        )

        # –ê–≤—Ç–æ—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ embedding –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
        try:
            auto_download = (
                os.getenv("AUTO_DOWNLOAD_EMBEDDING", "true").lower() == "true"
            )
            if auto_download:
                logger.info("üì• –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ embedding –º–æ–¥–µ–ª–∏...")
                import sys
                from pathlib import Path

                src_path = Path(__file__).parent.parent / "src"
                if str(src_path) not in sys.path:
                    sys.path.insert(0, str(src_path))

                from utils.model_downloader import download_embedding_model

                cache_dir = os.getenv("TRANSFORMERS_CACHE", "/models/.cache")
                download_embedding_model(embed_model, cache_dir)
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∞–≤—Ç–æ—Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å: {e}")

        collection = create_chroma_collection(args.collection, embed_model, device)

        logger.info(
            f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –Ω–∞ {device.upper()} —Å batch_size={batch_size}"
        )
        await ingest_batches(collection, msgs, batch_size)

        final_count = collection.count()
        logger.info("‚úÖ –ò–Ω–∂–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω. –ò—Ç–æ–≥–æ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: %s –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", final_count)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if len(msgs) > 0:
            logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(msgs)} —Å–æ–æ–±—â–µ–Ω–∏–π")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –∏–Ω–∂–µ—Å—Ç–∞: {e}")
        raise
    finally:
        await client.disconnect()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Interrupted by user")
